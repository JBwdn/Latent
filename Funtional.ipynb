{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\DR\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "import pandas as pd \n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "from synbioTools import tensorChem\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB.Polypeptide import d1_to_index\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\"\"\" Import_packages\"\"\"\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from IPython.display import SVG\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "from synbioTools import tensorChem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Read csv file and return it as a panda dataframe(dictionary) by biopython \"\"\"\n",
    "def readfasta(ffile):\n",
    "    \"\"\" Read fasta file, return dictionary \"\"\"\n",
    "    record_iterator = SeqIO.parse(ffile, \"fasta\")\n",
    "    a=pd.DataFrame(columns=['id','seq'])\n",
    "    for i in record_iterator:\n",
    "        a.loc[a.shape[0]+1] = [i.id,str(i.seq[:])] \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Read csv file and return it as a panda dataframe(dictionary) \"\"\"\n",
    "def read_csv(path):\n",
    "    \"\"\" Read csv, return a panda dataframe(dictionary) \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load chemicals information and convert the chemical info to SMILES format \"\"\"\n",
    "def load_chemicals(path):\n",
    "    \n",
    "    SolubilityData = read_csv(path) # read csv\n",
    "    chems=[] # variable to store the \n",
    "    \n",
    "    # change column names of \n",
    "    SolubilityData.rename(columns={ SolubilityData.columns[1]: \"Solubility\" }, inplace=True)\n",
    "    SolubilityData.rename(columns={ SolubilityData.columns[0]: \"Compound\" }, inplace=True)\n",
    "    SolubilityData.rename(columns={ SolubilityData.columns[2]: \"SMILES\" }, inplace=True)\n",
    "    \n",
    "    for row in range(0,len(SolubilityData['SMILES'])):\n",
    "        chems.append( Chem.MolFromSmiles(SolubilityData['SMILES'][row] ) )\n",
    "    SolubilityData['SMILES'] = chems\n",
    "    return SolubilityData # return the data list which contains the three input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Visualize seq length and solu \"\"\"\n",
    "# plot the histogram of solubility\n",
    "import re\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def visu_KDE(Values,name,path):\n",
    "    \n",
    "    if re.search(r'length',str(name)):\n",
    "        length=[]\n",
    "        for i in Values:\n",
    "            length.append(len(i))\n",
    "        train_y_plot = pd.Series( np.squeeze(length), name=name)\n",
    "    else:\n",
    "        train_y_plot = pd.Series( np.squeeze(Values), name=name)\n",
    "    mean = train_y_plot.mean()\n",
    "    std = train_y_plot.std()\n",
    "    print(\"The mean of the \"+name+\" is: \" + str(mean))\n",
    "    print(\"The S.D. of the \"+name+\" is: \" + str(std))\n",
    "    f,ax= plt.subplots(figsize = (14, 10))\n",
    "    sns.distplot(train_y_plot, kde=True, rug=True, hist=True)\n",
    "    ax.set_title(\"Density Plot of \"+name, fontsize=18, position=(0.5,1.05))\n",
    "    plt.savefig(path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\"\"\"Visualize seq vocab\"\"\"\n",
    "# Count amino acid and convert to vocab\n",
    "def vis_seq_elements(X_seq,path):\n",
    "    tmp=\"\"\n",
    "    for i in X_seq:\n",
    "        tmp = tmp + str(i)\n",
    "    c2 = Counter(tmp)\n",
    "    print(\"The frequency is: \" + str(c2))\n",
    "    print(\"Amino acids type is: \" + str(len(c2)))\n",
    "    print(\"They are: \" + str(c2.keys()))\n",
    "    k = pd.DataFrame.from_dict([c2])\n",
    "    classes=len(c2.keys())\n",
    "\n",
    "    f,ax= plt.subplots(figsize = (14, 10))\n",
    "    g=sns.barplot(data=k,ax=ax)\n",
    "    for p in g.patches:\n",
    "            g.annotate(\"%.0f\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', rotation=0, xytext=(0, 20), textcoords='offset points')\n",
    "    ax.set_title(\"Frequencies Hist of Amino Acids\", fontsize=18, position=(0.5,1.05))\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    return list(c2.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert SMILES into fingerprint\"\"\"\n",
    "def chemFP(chem, FINGERPRINT_SIZE, MIN_PATH=1, MAX_PATH=5):\n",
    "    fpix = AllChem.RDKFingerprint(chem, minPath=MIN_PATH, maxPath=MAX_PATH, fpSize=FINGERPRINT_SIZE)    \n",
    "    fpix = [int(x) for x in list(fpix.ToBitString())]\n",
    "    return fpix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Encode a chemical as a tensor by concatenating fingerprints up to desired depth \"\"\"\n",
    "def tensorChem(chems, FINGERPRINT_SIZE, CHEMDEPTH):\n",
    "    TRAIN_BATCH_SIZE = len(chems)   \n",
    "    Xs = np.zeros( (TRAIN_BATCH_SIZE, FINGERPRINT_SIZE, CHEMDEPTH) )\n",
    "    # print(Xs.shape)\n",
    "    for i in range(0, len(chems)-1):\n",
    "        for k in range(0, CHEMDEPTH):\n",
    "            fpix = chemFP(chems[i],FINGERPRINT_SIZE, k+1, k+1)\n",
    "            Xs[i, :, k] = fpix\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Flatten the tensor into a two dimentional vector(feature mapping) \"\"\"\n",
    "# The original vector shape\n",
    "def flatten_chem(X_chem):\n",
    "    \n",
    "    depth = 4\n",
    "    fpSize = 1024\n",
    "    tc = tensorChem(X_chem,fpSize, depth)\n",
    "    print('The original vector shape:\\n'+str(tc.shape))\n",
    "    # The flattened vector shape\n",
    "    train_x_flatten = tc.reshape(tc.shape[0], -1)\n",
    "    print('The flattened vector shape:\\n '+str(train_x_flatten.shape))\n",
    "    return train_x_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(seq):\n",
    "    \"\"\" Convert amino acid to one-hot vector stack \"\"\"\n",
    "    # Generate amino acids one-hot dict\n",
    "    to_one_hot = dict()\n",
    "    for i, a in enumerate(d1_to_index):\n",
    "        v = np.zeros(len(d1_to_index))\n",
    "        v[i] = 1\n",
    "        to_one_hot[a] = v\n",
    "    ix = []\n",
    "    result = []\n",
    "    # Tranfer the seq by the dict\n",
    "    for m in seq:\n",
    "        result.append(to_one_hot[m])\n",
    "    result = np.array(result)\n",
    "    return np.reshape(result, (1, result.shape[0], result.shape[1]))\n",
    "\n",
    "def index_seq(seq,vocab):\n",
    "    \"\"\" Convert amino acid to numerical index \"\"\"\n",
    "    index=[]\n",
    "    for i in seq:\n",
    "        if i in vocab: \n",
    "            p=vocab.index(i)\n",
    "            index.append(p)\n",
    "        else:\n",
    "            index.append('?')\n",
    "    index = index\n",
    "    return index\n",
    "\n",
    "def tensor_pad(seqs,vocab,max_length=False):\n",
    "    # Init seqs vector\n",
    "    seqs_index=[]\n",
    "    # Transfer seqs into index vector\n",
    "    for seq in seqs:\n",
    "        seqs_index.append(index_seq(seq,vocab))\n",
    "    # Pad the seqs\n",
    "    if max_length==False:\n",
    "        pad=pad_sequences(seqs_index, maxlen=None, dtype='int32',padding='pre', truncating='pre', value=0.)\n",
    "    else:\n",
    "        pad=pad_sequences(seqs_index, maxlen=max_length, dtype='int32',padding='pre', truncating='pre', value=0.)\n",
    "#     # one-hot encode the pad\n",
    "#     encoded = to_categorical(pad)\n",
    "\n",
    "    #return seqs_index\n",
    "    return pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Class to seq\"\"\"\n",
    "def catagorite_EC(EC):\n",
    "    tmp=[]\n",
    "    for i in EC:\n",
    "        i=i.split('.')\n",
    "        for k, a in enumerate(i):\n",
    "            if re.search(r'n',a):\n",
    "                a=re.search(r'\\d',a)[0]\n",
    "            i[k]=int(a)\n",
    "        if len(i)<=3:\n",
    "            i.append(0)\n",
    "        tmp.append(i)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Class to index\"\"\"\n",
    "from sklearn import preprocessing\n",
    "def catagorite_EC_index(EC):\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    encoder.fit(EC)\n",
    "    EC = encoder.transform(EC)\n",
    "    num_classes = np.max(EC) + 1\n",
    "\n",
    "    # Convert labels to one hot\n",
    "    EC = to_categorical(EC, num_classes)\n",
    "    return EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hardmax the labels \"\"\"\n",
    "# convert train_y into a vector range from 0 to 1\n",
    "def hardmax(Y_b4):\n",
    "    Y_b4=np.array([int(i) for i in Y_b4.T])\n",
    "    Y=np.zeros((1,len(Y_b4)))\n",
    "    mean = Y_b4.mean()\n",
    "    std = Y_b4.std()\n",
    "    for i in range(0,len(Y_b4)-1):\n",
    "        if (Y_b4[i] >=mean):\n",
    "            Y[0][i]=1\n",
    "        else:\n",
    "            Y[0][i]=0\n",
    "\n",
    "    print('There are '+ str(list(np.squeeze(Y)).count(1)) + ' soluble chemicals (positive samples) and ' + str(list(np.squeeze(Y)).count(0)) + ' insoluble chemicals (negative samples).')\n",
    "\n",
    "    # plot the input fingerprint length distribution plot\n",
    "    plt.plot(np.squeeze(Y))\n",
    "    plt.ylabel('solubility')\n",
    "    plt.xlabel('fingerprints')\n",
    "    plt.title(\"fingerprint and solubility distribution in binary classification\" )\n",
    "    plt.show()\n",
    "    \n",
    "    return np.squeeze(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NN Model\"\"\"\n",
    "from keras import backend as K\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# Customized R2 ACC method\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()))\n",
    "            \n",
    "#Try Adam as optimizer and implement time-based learning rate decay lr *= (1. / (1. + self.decay * self.iterations)\n",
    "         \n",
    "def create_network(layer_type=(\"LSTM\",[4096,32,32,1]),outputlayer_type='linear_regression',optimizer='Adam',Init='he_init',vocab=d1_to_index):\n",
    "    # Setup hyperparameters for optimizers\n",
    "    Adam = optimizers.Adam(lr=0.0075, beta_1=0.9, beta_2=0.999, epsilon=None, decay=1e-6, amsgrad=False)\n",
    "    sgd=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    \n",
    "    \n",
    "    # Start neural network\n",
    "    network = Sequential()\n",
    "    if layer_type[0]==\"LSTM\":\n",
    "        network.add(Embedding(output_dim=layer_type[1][1], input_dim=len(vocab), input_length=(layer_type[1][0])))\n",
    "        for i in layer_type[1][1:-1]: \n",
    "            network.add(LSTM(i, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "            network.add(Dropout(0.3))\n",
    "        #     network.add(LSTM(32))  # return a single vector of dimension 32\n",
    "        network.add(Flatten())\n",
    "    elif layer_type[0]==\"DFF\":\n",
    "        network.add(layers.Dense(units=layer_type[1][1], activation='relu', input_shape=(layer_type[1][0],)))\n",
    "        if len(layer_type[1]>=4):\n",
    "            for i in layer_type[1][2:-1]:\n",
    "                # Add fully connected layer with a ReLU activation function\n",
    "                network.add(layers.Dense(units=i, activation='relu'))\n",
    "\n",
    "    if outputlayer_type=='linear_regression':\n",
    "\n",
    "        # Add fully connected layer with a sigmoid activation function\n",
    "        network.add(layers.Dense(units=layer_type[1][-1]))\n",
    "\n",
    "        # Compile neural network\n",
    "        if optimizer == 'sgd':\n",
    "            network.compile(loss='mean_squared_error',optimizer= sgd,metrics=[coeff_determination]) # Accuracy performance metric-R2 sgd\n",
    "            print(\"Optimizer sgd; Loss mean_squared_error.\")\n",
    "        elif optimizer == 'Adam':\n",
    "            network.compile(loss='mean_squared_error',optimizer= Adam,metrics=[coeff_determination]) # Accuracy performance metric-R2 Adam\n",
    "            print(\"Optimizer Adam; Loss mean_squared_error.\")\n",
    "            \n",
    "    elif outputlayer_type=='binary_classifier':\n",
    "\n",
    "        # Add fully connected layer with a sigmoid activation function\n",
    "        network.add(layers.Dense(units=layer_type[1][-1], activation='sigmoid'))\n",
    "\n",
    "        # Compile neural network\n",
    "        if optimizer == 'sgd':\n",
    "            network.compile(loss='binary_crossentropy',optimizer= sgd,metrics=['accuracy']) # Accuracy performance metric sgd\n",
    "            print(\"Optimizer sgd; binary_crossentropy.\")\n",
    "        elif optimizer == 'Adam':\n",
    "            network.compile(loss='binary_crossentropy',optimizer= Adam,metrics=['accuracy']) # Accuracy performance metric Adam\n",
    "            print(\"Optimizer Adam; binary_crossentropy.\")\n",
    "        \n",
    "    elif outputlayer_type=='linear_regression':\n",
    "\n",
    "        # Add fully connected layer with a sigmoid activation function\n",
    "        network.add(layers.Dense(units=layer_type[1][-1]))\n",
    "\n",
    "        # Compile neural network\n",
    "        if optimizer == 'sgd':\n",
    "            network.compile(loss='mean_squared_error',optimizer= sgd,metrics=[coeff_determination]) # Accuracy performance metric-R2 sgd\n",
    "            print(\"Optimizer sgd; Loss mean_squared_error.\")\n",
    "        elif optimizer == 'Adam':\n",
    "            network.compile(loss='mean_squared_error',optimizer= Adam,metrics=[coeff_determination]) # Accuracy performance metric-R2 Adam\n",
    "            print(\"Optimizer Adam; Loss mean_squared_error.\")\n",
    "    \n",
    "    elif outputlayer_type=='multiple_classifier':\n",
    "\n",
    "        # Add fully connected layer with a softmax activation function\n",
    "        network.add(Dense(layer_type[1][-1], activation='softmax'))\n",
    "        # Compile neural network\n",
    "        if optimizer == 'sgd':\n",
    "            network.compile(loss='categorical_crossentropy',optimizer= sgd,metrics=['accuracy']) # Accuracy performance metric-R2 sgd\n",
    "            print(\"Optimizer sgd; Loss mean_squared_error.\")\n",
    "        elif optimizer == 'Adam':\n",
    "            network.compile(loss='categorical_crossentropy',optimizer= Adam,metrics=['accuracy']) # Accuracy performance metric-R2 Adam\n",
    "            print(\"Optimizer Adam; Loss mean_squared_error.\")\n",
    "    \n",
    "    # Return compiled network\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_models(Seq_NN,Seq_input,Chem_NN,Chem_input):\n",
    "    Seq_input = Input(shape=(Seq_input,))\n",
    "    encoded_seq = Seq_NN(Seq_input)\n",
    "    Chem_input = Input(shape=(Chem_input,))\n",
    "    encoded_chem = Chem_NN(Chem_input)\n",
    "    merged_NN = layers.concatenate([encoded_seq, encoded_chem])\n",
    "    merged_NN =  layers.Dense(units=1, activation='sigmoid')(merged_NN)\n",
    "    combined_model = Model([Seq_input, Chem_input], merged_NN)\n",
    "    combined_model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    return combined_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import math\n",
    "\n",
    "def get_r2_numpy(x, y):\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "    r_squared = 1 - (sum((y - (slope * x + intercept))**2) / ((len(y) - 1) * np.var(y, ddof=1)))\n",
    "    return r_squared\n",
    "\n",
    "def get_r2_scipy(x, y):\n",
    "    _, _, r_value, _, _ = stats.linregress(x, y)\n",
    "    return r_value**2\n",
    "\n",
    "def get_r2_statsmodels(x, y):\n",
    "    return sm.OLS(y, sm.add_constant(x)).fit().rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "def confusion_heatmap(Y_seq_EC_tokenized_index,Y_pre_EC,name):\n",
    "\n",
    "    tmp=[]\n",
    "    for i in Y_pre_EC:\n",
    "        num = np.argmax(i)\n",
    "        tmp.append(num)\n",
    "    Y_re=[np.argmax(i) for i in Y_seq_EC_tokenized_index]\n",
    "    Y_re=np.array(Y_re).reshape(np.array(Y_re).shape[0],)\n",
    "    tmp=np.array(tmp)\n",
    "\n",
    "    table=pd.crosstab(Y_re,tmp, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    table.to_csv(name + \"error_map.csv\")\n",
    "\n",
    "    cnf_matrix = confusion_matrix(Y_re, tmp)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=d2_to_index,title='Confusion matrix , without normalization')\n",
    "    plt.savefig(name + \"confusion_heatmap.svg\", bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot linear regression\n",
    "def linear_regression(Train_Y,Train_Y_pred,Test_Y,Test_Y_pred,path,i):\n",
    "        r_square=get_r2_statsmodels(Train_Y_pred,Train_Y)\n",
    "        q_square=get_r2_statsmodels(Test_Y_pred,Test_Y)\n",
    "        \n",
    "        # Plot Training-set\n",
    "        model=LinearRegression()\n",
    "        model.fit(Train_Y_pred,Train_Y)\n",
    "        ax=plt.gca()\n",
    "        plt.scatter(Train_Y_pred,Train_Y)\n",
    "        y_train_pred=model.predict(Train_Y_pred)\n",
    "        plt.title('Linear regression of solubility training prediction in fold '+str(i+1))\n",
    "        plt.text(0.5,0.7,\"The R-square value is %.2f \" % r_square, verticalalignment='bottom', horizontalalignment='right',transform=ax.transAxes)\n",
    "        plt.plot(Train_Y_pred,y_train_pred,color='black',linewidth=3,label=\"R-square\")\n",
    "        plt.legend(loc=2)\n",
    "        plt.xlabel(\"train  (mol/L)\")\n",
    "        plt.ylabel(\"theorical  (mol/L)\")\n",
    "\n",
    "        # Plot Testing-set\n",
    "        model=LinearRegression()\n",
    "        model.fit(Test_Y_pred,Test_Y)\n",
    "\n",
    "        plt.scatter(Test_Y_pred,Test_Y)\n",
    "        y_train_pred=model.predict(Test_Y_pred)\n",
    "\n",
    "        plt.text(0.5,0.645,\"The Q-square value is %.2f \" % q_square,verticalalignment='bottom',color='red', horizontalalignment='right',transform=ax.transAxes)\n",
    "        plt.plot(Test_Y_pred,y_train_pred,color='red',linewidth=3,label=\"Q-square\")\n",
    "        plt.legend(loc=2)\n",
    "        \n",
    "        plt.savefig(path+'linear_regression_fold_'+str(i+1)+'.svg', bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression INFO\n",
    "\n",
    "class error_ana_info():\n",
    "    def __init__(self):\n",
    "        self._train_metric = None\n",
    "        self._test_metric = None\n",
    "        self.df = None\n",
    "        self.mdif = None\n",
    "    \n",
    "    def add_info(self,train_metric,test_metric):\n",
    "        if self._train_metric is None or  self._test_metric is None:\n",
    "            self._train_metric = np.array(train_metric)\n",
    "            self._test_metric = np.array(test_metric)\n",
    "        else:\n",
    "            self._train_metric = np.vstack((self._train_metric, train_metric))\n",
    "            self._test_metric = np.vstack((self._test_metric, test_metric))\n",
    "    \n",
    "    def generate_csv(self,row_names,name):\n",
    "        if (self._train_metric.size!=0 and self._test_metric.size!=0):\n",
    "            _acc=[]\n",
    "            _acc_in_val_acc=[]\n",
    "            _valacc=[]\n",
    "            _epochs=[]\n",
    "            _train_err=[]\n",
    "\n",
    "            # Calculate max train_acc\n",
    "            for i in self._train_metric:\n",
    "                _acc.append(np.max(i))\n",
    "\n",
    "            # Calculate max test_acc and optimal_epo\n",
    "            for index,i in enumerate(self._test_metric):\n",
    "                _valacc.append(np.max(i))\n",
    "                _epochs.append(np.where(i==np.max(i,axis=0))[0][0]+1)\n",
    "                _acc_in_val_acc.append(self._train_metric[index][np.where(i==np.max(i,axis=0))[0][0]])\n",
    "            \n",
    "#            print(_acc,_acc_in_val_acc)\n",
    "            # Calculate max training_err\n",
    "            for i in range(len(_acc)):\n",
    "                _train_err.append('%.2f%%' %(((_acc[i]-_acc_in_val_acc[i])/_acc_in_val_acc[i])*100))\n",
    "                \n",
    "            self.df=[_acc,_acc_in_val_acc,_valacc,_epochs,_train_err]\n",
    "            self.df = pd.DataFrame(self.df)\n",
    "            self.df.set_index([row_names],inplace=True)\n",
    "            \n",
    "            # Calculate Average Value\n",
    "            meanv=[]\n",
    "            for index, row in self.df.iterrows():\n",
    "                \n",
    "                row=list(row)\n",
    "                if re.search(r'\\%',str(row[1])):\n",
    "                    row=[float(j.strip(\"%\")) for j in row]\n",
    "                    meanv.append(str(np.mean(row))+\"%\")\n",
    "                else:\n",
    "                    row=[float(j) for j in row]\n",
    "                    meanv.append(np.mean(list(row)))\n",
    "\n",
    "            self.df['B']=meanv\n",
    "            \n",
    "            col=[]\n",
    "            for i in range(1,len(self.df.iloc[0])):\n",
    "                col.append(\"Fold \"+str(i))\n",
    "            col.append('Average Value')\n",
    "            self.df.columns = col\n",
    "            self.df.to_csv(name)\n",
    "            \n",
    "    def generate_model_info(self,epochs,batch_size,initialization,bias,name):\n",
    "        Hyperparameters=pd.Series(data=['epochs','batch_size','initialization','bias'],name='Hyperparameters')\n",
    "        Settings=pd.Series(data=[epochs,batch_size,initialization,bias],name='Settings')\n",
    "        Settings.rename('Settings')\n",
    "        self.mdif=pd.DataFrame()\n",
    "        self.mdif= self.mdif.join(Hyperparameters, how='right')\n",
    "        self.mdif= self.mdif.join(Settings)\n",
    "        self.mdif.to_csv(name,index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training visualization function\n",
    "def training_vis(hist,outputlayer,name):\n",
    "    loss = hist.history['loss']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    if outputlayer=='linear_regression':\n",
    "        acc = hist.history['coeff_determination']\n",
    "        val_acc = hist.history['val_coeff_determination']\n",
    "    elif outputlayer=='binary_classifier':\n",
    "        acc = hist.history['acc']\n",
    "        val_acc = hist.history['val_acc']\n",
    "    elif outputlayer=='multiple_classifier':\n",
    "        acc = hist.history['acc']\n",
    "        val_acc = hist.history['val_acc']\n",
    "\n",
    "    # make a figure\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    # subplot loss\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1.plot(loss,label='train_loss')\n",
    "    ax1.plot(val_loss,label='val_loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Loss on Training and Validation Data')\n",
    "    ax1.legend()\n",
    "    # subplot acc\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    if outputlayer=='linear_regression':\n",
    "        ax2.plot(acc,label='train_coeff_determination')\n",
    "        ax2.plot(val_acc,label='val_coeff_determination')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Coeff_determination')\n",
    "        ax2.set_title('Coeff_determination  on Training and Validation Data')\n",
    "    elif outputlayer=='binary_classifier':\n",
    "        ax2.plot(acc,label='train_accuracy')\n",
    "        ax2.plot(val_acc,label='val_accuracy')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Accuracy  on Training and Validation Data')\n",
    "    elif outputlayer=='multiple_classifier':\n",
    "        ax2.plot(acc,label='train_accuracy')\n",
    "        ax2.plot(val_acc,label='val_accuracy')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Accuracy  on Training and Validation Data')\n",
    "    \n",
    "    ax2.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(name, bbox_inches='tight')\n",
    "    plt. close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \"\"\"StratifiedKFold\"\"\"\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def k_folds_NN(network=None, X=None, Y=None, batch_size=50, epochs=5, path='C:/Users/DR/Desktop/P2/Latent-master/pic/Seq_linear_regression_dropout_1138/', Init='he_init', outputlayer_type='binary_classifier'):\n",
    "    # Store training info\n",
    "    info=error_ana_info()\n",
    "\n",
    "    # Split the dataset in 3 folds\n",
    "    sfolder = KFold(n_splits=3,random_state=0,shuffle=True)\n",
    "    sfolder.get_n_splits(X,Y)\n",
    "    \n",
    "    # If directory dosn't exixst, then create directory.  \n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "#    early_stopping = EarlyStopping(monitor='val_loss',min_delta=0,patience=8,verbose=0, mode='auto')## Callback for early stopping the training\n",
    "    \n",
    "    #K-folds iteration    \n",
    "    for i, (train, test) in enumerate(sfolder.split(X,Y)):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = Y[train], Y[test]\n",
    "        \n",
    "        # Train the model with each combination of folds\n",
    "        \n",
    "        hist = network.fit(X_train, y_train,batch_size=batch_size, epochs=epochs, validation_data=(X_test,y_test))\n",
    "        \n",
    "        if outputlayer_type=='linear_regression':\n",
    "            # Plot Linear Regression\n",
    "            info.add_info(hist.history['coeff_determination'],hist.history['val_coeff_determination'])\n",
    "            linear_regression(Train_Y=y_train,Train_Y_pred=network.predict(X_train),Test_Y=y_test,Test_Y_pred=network.predict(X_test),path=path,i=i)\n",
    "        elif outputlayer_type=='binary_classifier':\n",
    "            info.add_info(hist.history['acc'],hist.history['val_acc'])\n",
    "        elif outputlayer_type=='multiple_classifier':\n",
    "            info.add_info(hist.history['acc'],hist.history['val_acc'])\n",
    "            confusion_heatmap(y_test,network.predict(X_test),path + \"fold_\" + str(i+1) + \"_\")\n",
    "            \n",
    "    # hist = network.fit(X_train,  y_train,batch_size=batch_size, epochs=epochs, validation_data=(X_test,y_test), callbacks=[early_stopping])\n",
    "              \n",
    "        # Plot runtime\n",
    "        training_vis(hist,outputlayer_type,path+\"train_fold \"+str(i+1)+\".svg\")\n",
    "        \n",
    "        # Save NN Model\n",
    "        network.save(path+'batch_size_'+str(batch_size)+'epochs_'+str(epochs)+'fold_'+str(i+1)+'.h5')\n",
    "        i=i+1\n",
    "        \n",
    "    # Output training information\n",
    "    if outputlayer_type=='linear_regression':\n",
    "        info.generate_csv(['R2','R2_opt','Q2','Epochs_opt','Train_err'],path+\"training_result.csv\")\n",
    "    elif outputlayer_type=='binary_classifier':\n",
    "        info.generate_csv(['Train_Acc','Train_Acc_opt','Test_Acc','Epochs_opt','Train_err'],path+\"training_result.csv\")\n",
    "    elif outputlayer_type=='multiple_classifier':\n",
    "        info.generate_csv(['Train_Acc','Train_Acc_opt','Test_Acc','Epochs_opt','Train_err'],path+\"training_result.csv\")\n",
    "    \n",
    "    if Init=='he_init':\n",
    "        info.generate_model_info(epochs,batch_size,\"He_Init\",\"Enabled\",path+'model_info.csv')\n",
    "    elif Init=='random_no_bias':\n",
    "        info.generate_model_info(epochs,batch_size,\"Random\",\"False\",path+'model_info.csv')\n",
    "    elif Init=='random_with_bias':\n",
    "        info.generate_model_info(epochs,batch_size,\"Random\",\"True\",path+'model_info.csv')\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set working directory\"\"\"\n",
    "# If directory dosn't exixst, then create directory.   \n",
    "path=\"C:/Users/DR/Desktop/P2/Latent-master/pic/Functional_Test/\"\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load Data\"\"\"\n",
    "# Load seqs_thermoatability\n",
    "non_thermophilic_proteins = readfasta(\"C:/Users/DR/Desktop/P2/Latent-master/data/thermostability/non-thermophilic_proteins.txt\")\n",
    "thermophilic_proteins = readfasta(\"C:/Users/DR/Desktop/P2/Latent-master/data/thermostability/thermophilic_proteins.txt\")\n",
    "thermophilic_proteins['Thermostability']='1'\n",
    "non_thermophilic_proteins['Thermostability']='0'\n",
    "# Append the thermophilic to the non-thermophilic\n",
    "sequence_data=pd.concat([thermophilic_proteins,non_thermophilic_proteins])\n",
    "sequence_data=sequence_data.sample(frac=1)\n",
    "# Init input and Y\n",
    "X_seq=sequence_data['seq']\n",
    "Y_seq=sequence_data['Thermostability']\n",
    "\n",
    "# Load chems_solubility\n",
    "chem_data = load_chemicals(\"C:/Users/DR/Desktop/P2/Latent-master/data/solubility/delaney.csv\")\n",
    "X_chem=chem_data['SMILES']\n",
    "Y_chem=np.array(chem_data['Solubility'])\n",
    "\n",
    "# Load EC data\n",
    "EC=read_csv(\"C:/Users/DR/Desktop/P2/Latent-master/data/ec/ecseq.csv\")\n",
    "EC.drop(['Uniprot'], axis=1,inplace=True)\n",
    "EC = EC[pd.notnull(EC['EC'])]\n",
    "EC = EC[pd.notnull(EC['Sequence'])]\n",
    "X_seq_EC=EC['Sequence']\n",
    "Y_seq_EC=EC['EC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize data\"\"\"\n",
    "# If directory dosn't exixst, then create directory.   \n",
    "path=\"C:/Users/DR/Desktop/P2/Latent-master/pic/Functional_Test/\"\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "# Visualize the solubility and seq_length distribution\n",
    "visu_KDE(Y_chem,\"chem solubility\",path+'Density Plot of chem solubility.svg')\n",
    "visu_KDE(X_seq,\"seq length\",path+'Density Plot of seqs length.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tokenize data\"\"\"\n",
    "# Generate vocab\n",
    "d1_to_index=vis_seq_elements(X_seq,path+'Frequencies Hist of Amino Acids Thermostability.svg')\n",
    "d2_to_index=vis_seq_elements(X_seq_EC,path+'Frequencies Hist of Amino Acids EC.svg')\n",
    "\n",
    "# Tokenlize chem\n",
    "X_chem=flatten_chem(X_chem)\n",
    "# Tokenlize seq_thermostability\n",
    "X_seq=tensor_pad(X_seq,d1_to_index)\n",
    "# Tokenlize seq_EC\n",
    "X_seq_EC=tensor_pad(X_seq_EC,d2_to_index,max_length=200)\n",
    "# Tokenize EC\n",
    "Y_seq_EC_tokenized_index=catagorite_EC_index(Y_seq_EC)\n",
    "Y_seq_EC_tokenized=catagorite_EC(Y_seq_EC)\n",
    "# Hardmax solubility\n",
    "Y_chem_hardmax=hardmax(Y_chem)\n",
    "# Hardmax thermostability\n",
    "Y_seq_hardmax=hardmax(Y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Build NN\"\"\"\n",
    "# Build two sequential NN\n",
    "Seq_NN=create_network(layer_type=(\"LSTM\",[32,32]),outputlayer_type='binary_classifier',optimizer='Adam',number_of_features = X_seq.shape[1],Init='he_init')\n",
    "Chem_NN=create_network(layer_type=(\"DFF\",[20,7,5]),outputlayer_type='binary_classifier',optimizer='Adam',number_of_features =X_chem.shape[1],Init='he_init')\n",
    "Seq_NN.summary()\n",
    "Chem_NN.summary()\n",
    "\n",
    "# Build functional API combined NN\n",
    "combine_model = combine_models(Seq_NN,X_seq.shape[1],Chem_NN,X_chem.shape[1])\n",
    "combine_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chem->properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Solubility prediction\"\"\"\n",
    "# Build two sequential NN\n",
    "Chem_NN=create_network(layer_type=(\"DFF\",[X_chem.shape[1],20,7,5,1]),outputlayer_type='linear_regression',optimizer='Adam',Init='he_init')\n",
    "Chem_NN.summary()\n",
    "hist=k_folds_NN(network=Chem_NN,X=X_chem,Y=Y_chem,batch_size=1144, epochs=107,path='C:/Users/DR/Desktop/P2/Latent-master/pic/Optimal/Chem_properties/',Init='he_init',outputlayer_type='linear_regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq->properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chem/Seq->binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Solubility prediction\"\"\"\n",
    "# Build two sequential NN\n",
    "Chem_NN=create_network(layer_type=(\"DFF\",[X_chem.shape[1],20,7,5,1]),outputlayer_type='linear_regression',optimizer='Adam',Init='he_init')\n",
    "Chem_NN.summary()\n",
    "hist=k_folds_NN(network=Chem_NN,X=X_chem,Y=Y_chem_hardmax,batch_size=1144, epochs=1,path='C:/Users/DR/Desktop/P2/Latent-master/pic/Optimal/Chem_binary classification/',Init='he_init',outputlayer_type='binary_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Thermostability prediction\"\"\"\n",
    "# Build two sequential NN\n",
    "Seq_NN=create_network(layer_type=(\"LSTM\",[X_seq.shape[1],32,32,1]),outputlayer_type='binary_classifier',optimizer='Adam' ,Init='he_init')\n",
    "Seq_NN.summary()\n",
    "hist=k_folds_NN(network=Seq_NN,X=X_seq,Y=Y_seq_hardmax,batch_size=6, epochs=1,path='C:/Users/DR/Desktop/P2/Latent-master/pic/Optimal/Seq_binary classification/',Init='he_init',outputlayer_type='binary_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq->multiple classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer Adam; Loss mean_squared_error.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 200, 32)           768       \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 200, 32)           8320      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 200, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 200, 32)           8320      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 200, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 502)               3213302   \n",
      "=================================================================\n",
      "Total params: 3,230,710\n",
      "Trainable params: 3,230,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33466 samples, validate on 16734 samples\n",
      "Epoch 1/50\n",
      "33466/33466 [==============================] - 129s 4ms/step - loss: 2.6576 - acc: 0.5246 - val_loss: 1.6320 - val_acc: 0.6603\n",
      "Epoch 2/50\n",
      "33466/33466 [==============================] - 126s 4ms/step - loss: 1.2629 - acc: 0.7094 - val_loss: 1.4747 - val_acc: 0.6770\n",
      "Epoch 3/50\n",
      "33466/33466 [==============================] - 126s 4ms/step - loss: 0.9500 - acc: 0.7553 - val_loss: 1.4489 - val_acc: 0.6830\n",
      "Epoch 4/50\n",
      "33466/33466 [==============================] - 126s 4ms/step - loss: 0.7709 - acc: 0.7904 - val_loss: 1.4610 - val_acc: 0.6832\n",
      "Epoch 5/50\n",
      "33466/33466 [==============================] - 126s 4ms/step - loss: 0.6615 - acc: 0.8123 - val_loss: 1.4452 - val_acc: 0.6832\n",
      "Epoch 6/50\n",
      " 1152/33466 [>.............................] - ETA: 1:50 - loss: 0.4426 - acc: 0.8715"
     ]
    }
   ],
   "source": [
    "\"\"\"Thermostability prediction\"\"\"\n",
    "# Build two sequential NN\n",
    "Seq_NN_multi=create_network(layer_type=(\"LSTM\",[X_seq_EC.shape[1],32,32,502]),outputlayer_type='multiple_classifier',optimizer='Adam',Init='he_init',vocab=d2_to_index)\n",
    "# Seq_NN_multi=create_network(layer_type=(\"LSTM\",[X_seq_EC.shape[1],1,502]),outputlayer_type='multiple_classifier',optimizer='Adam',Init='he_init',vocab=d2_to_index)\n",
    "Seq_NN_multi.summary()\n",
    "hist=k_folds_NN(network=Seq_NN_multi,X=X_seq_EC,Y=Y_seq_EC_tokenized_index,batch_size=128, epochs=50,path='C:/Users/DR/Desktop/P2/Latent-master/pic/Optimal/Seq_multiple classification/',Init='he_init',outputlayer_type='multiple_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chem + Seq->combined NN->binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
